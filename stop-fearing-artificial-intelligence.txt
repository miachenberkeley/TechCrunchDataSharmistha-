Editors note:Tim Oates is chief scientist at CircleBack. He holds a PhD in computer science with an emphasis in machine learning from UMass Amherst and is also Oros Family Professor of Computer Science and Electrical Engineering at University of Maryland Baltimore County.As yet another tech pioneer with no connection to artificial intelligence steps out to voice his fears about AI being catastrophic for the human race, I feel the need respond. While I respect Steve Wozniaks technological contributions to our culture, I fear that he, like so many others (Musk, Hawking, Gates), is poisoning the well for fear of something he doesnt truly understand.Conflating facts of technologys rapid progress with a Hollywood understanding of intelligent machines is provocative (honestly, its a favorite in my most-loved science fiction books and movies), but this technology doesnt live in a Hollywood movie, it isnt HAL or Skynet, and it deserves a grounded, rational look.For the sake of argument, lets assume that we have (or can plausibly) create a superhuman AI. Such an AI could, like us, think all kinds of things  the humans created me and theyre really interesting or the humans bodily functions are mildly annoying or all humans must die!  all of which are equally speculatively plausible. So why anyone gives the doomsday scenario any more weight than the others is a bit of a mystery to me.It may be that, in a world filled with pop culture stories and polluted by a fear of tech, the doomsday story is the most entertaining, taking its spot next to UFO-created crop circles and the like. But the assumptions that this story-presented-as-an-idea rests on are unfounded and highly improbable.Heres what youre supposed to believe about true AI:Sound reasonable to you? Me either.But for claritys sake, lets unpack these assumptions, starting with the notion that AI has a distinct I capable of stepping outside its intended programming. Even the quickest glance over the history of AI confirms theres a tradeoff between machine intelligence and adaptability.Narrowly intelligent machines like Deep Blue and Watson can play chess or answer Jeopardy questions better than anyone alive while not being able to understand checkers or the Trivia Crack app. More general-intelligence machines, on the other hand, can learn to do many things but will ultimately do them all poorly.For example, the Association for the Advancement of Artificial Intelligencehosted a competition on general game playing, where a program is given the rules of a game and asked to play. The entrants could play lots of game-types after reading the rules  board games, card games, strategy games  but they played them all poorly.What Im getting at here is that an AI thats really good at, say, designing individualized cancer drugs isnt usually well-suited for other tasks. Deep Blue cant play checkers because it cant mentally represent or reason about checkers, and all other single-purpose AIs have similar mental gaps. And AIs with broad knowledge? They probably wont be very good at anything (including world domination).But lets suppose, for a second, that an AI does learn to think intelligently outside its programming and that its become discontent. Would this superhuman intelligence inherently go nuclear, or would it likely just slack off a little at work or, in extreme cases, compose rap music in Latin? In a world filled with a nearly infinite number of things a thinking entity can do to placate itself, its unlikely destruction of humanity will top any AIs list.But lets assume that it has, that weve built an AI that learns to think outside its programming, that its become discontent, and that its bent on world domination. As you can imagine, world domination isnt a trivial endeavor; it requires resources with global reach. And, because were not making the Terminator error and giving unfettered control over all military might to a computer program, well have to assume that, more plausibly, we may find ourselves dealing with computer control over financial systems, communication infrastructure, and the like.Even if this were the case, there is absolutely no reason to believe that, by virtue of running on a computer, an AI will be better at computers than we are. In fact, heated debate inspired by mathematical logic theorems suggests just the opposite. Just like living in a house doesnt transform you into a carpenter, being hosted on a computer doesnt guarantee a super-sophisticated understanding of them.Ive been reading and thinking about AI since I was a child, and Ive been working professionally in the field full-time since entering grad school more than 20 years ago. Im as excited today as I was at the age of 10 about AI, and not because I think itll enable me some fetishistic power over the world, or because itll allow me to become part of the singularity or any of those other thrilling stories.Instead, Im excited by AI because of what it might tell us about what it means to be human  about how we might speed up the process of solving some of the worlds most pressing problems.I do believe well create a truly intelligent machine at some point. Not in my lifetime, but eventually. What we shouldnt do is spend the mean time telling scary stories. Leave that to the novelists and filmmakers. Please.