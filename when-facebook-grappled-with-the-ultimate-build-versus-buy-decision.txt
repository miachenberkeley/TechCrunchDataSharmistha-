At some point, as companies mature, they face a build versus buy technology decision.A company like Facebook encountersthat kind of choice constantly, but back in the 2009/2010 timeframe, it had an extraordinary one.It was growing faster than just about any company on the planet and was having issues keeping up with that scale. Thats when it decided it had to start designing its own hardware and building its own data centers to meet the companys very specific needs.At thattime,Facebook was purchasing equipment through the usual channels and placing it in leased co-location facilities, but it was finding commercial equipment, even from the most reputable manufacturers wasnt flexible enough to meet its needs, Jay Parikh vice president of infrastructure engineering at Facebook told TechCrunch earlier this month.Parikh, who is in charge of developing the software and hardware infrastructure that runs Facebook spoke candidly about the challenges and choices his company eventually made. Itwas a significantdecision involving a major investment while changing the way the entire companyworked.Yet Facebookwas able to make the transition remarkably fast  and it hasnt looked back.The company reached the build-buy crossroads essentially by accident because it was growing too quickly. It was constantly running into obstacles as a result of that growth and it was having an impact on the business.We were having to slow down products and features for the business because we didnt have performance characteristics we needed when they were all off-the-shelf components, Parikh said.We were having to slow down products and features for the business because we didnt have performance characteristics we needed when they were all off-the-shelf components. Jay ParikhAs Facebookgrew, it required getting under the hood of this equipment and making changes, but the nature of proprietary equipment made this extremely challenging. Thecompany had to debug different things that other people had writtenand the deeper it got into the process, the harder it became to understand the lowest levels of infrastructure.We could [have kept]going down the [same] path and buying stuff and making it all work, but it was error prone, not flexible, costly and hard to troubleshoot, he explained.It was at this point, the company made the build decision.In 2009/2010 the first thing that happened was the realization that we werent going to keep up and be flexible and perform and be at right cost structure. In order to get flexible and [control] the cost metrics, it forced us to go build ourselves, Parikh said.Once Facebookwas in control of its own destiny, it enabledthe company to approachhardwarein completely newways. It was no longerbound byold rules about the physical design of the equipment. It could abandon preconceived notions, which engineers have developed over the years to provide a standard way of storing the equipment. When the company wasdesigning andbuilding the racks and the equipment, it gave engineers the power to experiment and rethink every aspect of the design.And thats precisely what itsdone. As Facebooks hardware lab, director of engineering Matt Corddry told TechCrunch last year, Facebook knows its own requirements better than anyone:We understand our challenges, costs, operating environment and needs better than an outside vendor and we are able to specialize on the specific needs of Facebook, he explained at the time.Since it made its decision, Facebook has designed a range of equipment such as networking top of rack switches, which enables the company to programmatically control every part of the equipment, giving it tremendous flexibility.At the same time, Facebook began developing software to manage these custompieces of equipment suchas FBoss Agent, the software the company created to run those custom top of rack switches.Finally, it designed highly efficient spaces to house that equipment such as the one it opened in Altoona, Iowa last Fall. Facebook looked at every aspect of the data center design from how it was cooled  using 100 percent outside air instead of expensive air conditioning systems  to the electrical equipment and the racks that housed the equipment.AfterFacebook began designing its own equipment and data centers, it made another decision to bring the power of the community to bear on the problem by open sourcing not just the software, but also its hardware designs. It launched the Open Compute Project Foundation, which is an organization created by Facebook to help share these designs.According to the organizations mission statement, The Open Compute Project Foundation is a rapidly growing community of engineers around the world whose mission is to design and enable the delivery of the most efficient server, storage and data center hardware designs for scalable computing.Its not surprising that the statement in many ways mirrors the mission of Facebook itself. The purpose is clearly to give others the chance to take advantage of Facebooks hardware designs with the goal of advancingscalable computing, while helping Facebook improve its designs.Its a situation where everybody should win.Facebook started the project 4 years ago because it recognized other companieshad similar problems related to scale and it would be efficient to work together. We wanted to bring together a group to share a common set of problems and come together to solve them, Parikh said.Last year there were 1,000 engineers who contributed to open source projects Facebook started but who dont work at Facebook, he said.In spite of the speed in which Facebook made this transition, it would be a mistake to think it did so willy nilly. Facebook had a plan and it relied ondata to make sure it was achieving the desired results. At the macro level, data is embedded in every decision Facebook makes, Parkih explained.When we started down the path [to building our own hardware], we dipped our feet in, Parikh said. We only designed and built one server. We didnt do all of the configurations. We started with the simplest design from a hardware perspective, building a web server.Along the same lines, it built just one data center and grew from there.The company constantly reviews the data. If it turns out it makes more sense to buy than build, it does that, and its something the company is constantly evaluating. It also looks to in-house expertise and asks people who know the most about a particular problem and takes that into consideration, Parikh explained.We want to empower technology leads to get the data they need to drive their parts of their business, he said.In spite of all these decisions, tests and large-scale transition, Parkih made it sound like it wasnt that big a deal. Its not rocket science. We know what our strategy is and we look at the data. Sounds pretty simple, right?